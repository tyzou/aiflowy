# 大模型

![llmNode.png](../resource/llmNode.png)

## 模型列表

模型列表的数据来自大模型管理页面，具体请参考[大模型管理](../../llm/addLlm.md)

![llmList.png](../resource/llmList.png)

## 采样参数

### Temperature
> 作用：调整模型对概率分布中低概率词的敏感度，控制生成的随机性。
- 低温度（如 0.1~0.5）：

模型更倾向于选择概率最高的词，输出更确定、保守，适合需要严谨性的任务（如问答、代码生成）。

概率分布会变得更“尖锐”，高概率词被进一步放大。

- 高温度（如 0.7~1.2）：

模型会更多考虑低概率词，输出更随机、有创意，适合开放生成（如诗歌、故事）。

概率分布更“平滑”，低概率词有机会被选中。
### Top P
> 作用：动态地从累积概率超过 P 的最小词集合中采样，避免固定 K 的缺陷。

- P 值小（如 0.3~0.7）：词集合窄，生成更保守。

- P 值大（如 0.8~0.95）：词集合宽，生成更多样。

- 优势：自适应上下文。

    - 若概率分布集中（如某词概率 0.9），可能仅选 1 个词。

    - 若概率分布平坦（如 10 个词各 0.05），可能选更多词。
### Top K
> 作用：限制模型每一步只从概率最高的 K 个词中采样，排除低概率词。

- K 值小（如 10~50）：生成更保守，避免生僻词，但可能重复。

- K 值大（如 100~500）：多样性更高，但可能包含不合理词。

### 三者的关系与典型用法
**通常联合使用：**
  - 先通过 Top-K 或 Top-P 缩小候选词范围，再用 Temperature 调整概率分布。
  - 例如：Top-P=0.9, Temperature=0.7 是常见组合。

**区别：**
  - Temperature 直接调整概率分布形状。
  - Top-K/Top-P 是采样策略，限制候选词范围。

**推荐配置：**
  - 确定性任务：低 Temperature + 低 Top-P（如 T=0.3,P=0.5）。
  - 创造性任务：高 Temperature + 高 Top-P（如 T=1.0,P=0.9）。
## 系统提示词
> 作用：定义模型的身份、行为准则和回复风格，是对话的“幕后指令”。
> 在节点中可使用 <span v-pre>{{propertyName}}</span> 语法引用变量数据。

**特点：**

- 隐藏性：用户通常看不到（如API中的system角色或后台配置）。

- 全局性：影响整个对话的基调，而非单次回复。

- 功能性：用于约束模型输出（如安全性、专业性、语气等）。

**常见用途：**
- 设定角色：
```
"你是一位严谨的医学专家，用专业术语回答健康问题，避免非科学建议。"
```
- 安全限制：
```
"拒绝回答涉及暴力、隐私或违法内容。"
```
- 输出格式：
```
"所有回答用Markdown格式，包含标题和列表。"
```
## 用户提示词

> 作用：提供具体的输入问题或指令，是模型生成回复的直接依据。
> 在节点中可使用 <span v-pre>{{propertyName}}</span> 语法引用变量数据。

**特点：**

- 可见性：用户主动输入的内容（如聊天框中的提问）。

- 局部性：仅影响当前轮次的回复。

- 灵活性：可随时改变话题或风格。

**常见类型：**
- 直接提问：
```
"Java如何实现快速排序？"
 ```
- 复杂指令：
```
"用莎士比亚的风格写一首关于咖啡的诗。"
```